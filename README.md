## Realistic texture generation on 3D meshes using Diffusion models from single RGB image

*Thesis submitted for the degree of Master of Science in Artificial Intelligence at KU Leuven, specialisation Engineering and Computer Science.*

### Abstract
Texture creation plays an essential role in 3D design, with extensive applications in virtual reality (VR), extended reality (XR), and game development. However, traditional texture creation workflows are highly labor-intensive, often involving manual painting or multi-view image capture. Recent approaches leverage deep generative models to automate this process, yet producing textures that are both visually consistent and semantically stable across multiple views, particularly for complex 3D shapes, remains a significant challenge. In this thesis, we introduce Pose-Aware Paint3D (PA-Paint3D), which enhances an inpainting-based texture generation pipeline by replacing its canonical-view synthesis stage with a fine-tuned multi-view diffusion model. The pipeline first synthesizes multiple 2D views from specified camera viewpoints using a pre-trained diffusion model, projects them onto the UV texture map progressively, and fills missing regions through inpainting. Our diffusion model is fine-tuned on a multi-view dataset while injecting camera pose and directional cues as additional conditioning. To demonstrate that our approach could mitigate multi-view inconsistencies and improve semantic stability, we conduct comprehensive experiments on two tasks. For multi-view image generation, our fine-tuning strategy reduces the FID by $23.5\%$ and KID by $63.5\%$ compared to the unfine-tuned baseline. For the downstream texture generation task, our PA-Paint3D achieves up to an $8.7\%$ FID improvement and $16.3\%$ KID improvement over the original pipeline. Nevertheless, we acknowledge that the method still struggles to produce fully seamless and visually plausible textures, especially for meshes with complex or highly layered geometries.

### Acknowledgment
Our implementation builds upon and relies on the publicly available codebases of [Paint3D](https://github.com/OpenTexture/Paint3D) and [Point-UV-Diffusion](https://github.com/CVMI-Lab/Point-UV-Diffusion). We sincerely thank the authors for open-sourcing their work, which has been instrumental to the development of our approach.

The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government.